{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import metrics as metric\n",
    "from warnings import filterwarnings\n",
    "import pdb\n",
    "import os \n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import *\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_options = ['arousal', 'valence']\n",
    "partition_info = pd.read_csv('/media/sven/New Volume/features/meta/processed_tasks/metadata/partition.csv')\n",
    "classes = [0, 1, 2]\n",
    "feature_set = 'egemaps'\n",
    "\n",
    "\n",
    "feat_conf = {'egemaps': (88, 1, ',', 'infer'),\n",
    "             'deepspectrum': (4096, 1, ',', 'infer'),\n",
    "             'vggface': (512, 1, ',', 'infer'),\n",
    "             'fasttext': (300, 1, ',', 'infer'),\n",
    "             'xception': (2048, 1, ',', 'infer'),\n",
    "             'openpose': (54, 1, ',', 'infer')\n",
    "            }\n",
    "num_feat = feat_conf[feature_set][0]\n",
    "ind_off  = feat_conf[feature_set][1]\n",
    "sep      = feat_conf[feature_set][2]\n",
    "header   = feat_conf[feature_set][3]                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_if_need(paths, size):\n",
    "    diff = len(paths) - size\n",
    "    if diff < 0:\n",
    "        if abs(diff) > len(paths): # lon hon nhieu hon 2 lan so voi paths nen co the lap lai path\n",
    "            up_sampling = paths[np.random.choice(paths.shape[0], abs(diff), replace=True)]\n",
    "        else: # lon hon khong qua 2 lan so voi paths nen de khong lap lai path\n",
    "            up_sampling = paths[np.random.choice(paths.shape[0], abs(diff), replace=False)]\n",
    "        paths = np.concatenate([paths, up_sampling])\n",
    "    return paths\n",
    "\n",
    "def pooling_segment(video_features: np.array, stride=2, pool_out=16):\n",
    "    if len(video_features) < pool_out:\n",
    "        video_features = pad_if_need(video_features, pool_out)\n",
    "        return [video_features]\n",
    "    else:\n",
    "        ret = []\n",
    "        i = 0\n",
    "\n",
    "        while (i < int(len(video_features)) - pool_out):\n",
    "            ret.append(video_features[i:i + pool_out])\n",
    "            i += stride\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_segments(feature_df: pd.DataFrame, label_df:pd.DataFrame, stride=2, pool_out=16):\n",
    "    col_features = [col for col in feature_df.columns if not col in ['timestamp', 'segment_id']]\n",
    "    all_segments = feature_df['segment_id'].unique()\n",
    "    all_segment_features = []\n",
    "    all_segment_labels = []\n",
    "    all_segment_ids = []\n",
    "    for segment in all_segments:\n",
    "        segment_df = feature_df[feature_df['segment_id'] == segment]\n",
    "        segment_features = segment_df[col_features].values\n",
    "        label = label_df[label_df['segment_id'] == segment]['class_id'].values\n",
    "        pooled_feature = pooling_segment(segment_features, stride, pool_out)\n",
    "        pooled_label = [label] * len(pooled_feature)\n",
    "        segment = [segment] * len(pooled_feature)\n",
    "        all_segment_features += pooled_feature\n",
    "        all_segment_labels += pooled_label\n",
    "        all_segment_ids += segment\n",
    "    all_segment_features = np.asarray(all_segment_features)\n",
    "    all_segment_labels = np.asarray(all_segment_labels)\n",
    "    all_segment_ids = np.asarray(all_segment_ids)\n",
    "    return all_segment_features, all_segment_labels, all_segment_ids\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    for label in ['arousal']:\n",
    "        if not os.path.exists('./data_csv/'+label):\n",
    "            os.makedirs('./data_csv/'+label)\n",
    "        train_lab, train_feat, train_id, devel_lab, devel_feat, devel_id, test_lab, test_feat , test_id= [], [], [], [], [], [], [] ,[], []\n",
    "\n",
    "        feature_folder = '/media/sven/New Volume/features/c2_muse_topic/feature_segments/egemaps_aligned/'\n",
    "        label_folder = '/media/sven/New Volume/features/c2_muse_topic/label_segments/' + label + '/'\n",
    "\n",
    "        print('\\n ' + feature_set + ': ' + label)\n",
    "\n",
    "        print('\\n Preparing Partitions')\n",
    "        for index, row in tqdm(partition_info.iterrows()):\n",
    "            filename_id = str(row['Id']) + '.csv'\n",
    "            row_partition = row['Proposal']\n",
    "\n",
    "            label_df = pd.read_csv(label_folder + filename_id, index_col=None, dtype=np.float64)\n",
    "            feature_df = pd.read_csv(feature_folder + feature_set + '/' + filename_id, index_col=None, dtype=np.float64)\n",
    "            features, labels, id_segment = pooling_segments(feature_df, label_df)\n",
    "\n",
    "            if row_partition == 'train':\n",
    "                train_lab.append(labels)\n",
    "                train_feat.append(features)\n",
    "                train_id.append(id_segment)\n",
    "            if row_partition == 'devel':    \n",
    "                devel_lab.append(labels)\n",
    "                devel_feat.append(features)\n",
    "                devel_id.append(id_segment)\n",
    "            if row_partition == 'test':\n",
    "                test_lab.append(labels)\n",
    "                test_feat.append(features)\n",
    "                test_id.append(id_segment)\n",
    "            \n",
    "            \n",
    "            list_value = [train_lab, train_feat, train_id, devel_lab, devel_feat, devel_id, test_lab, test_feat , test_id]\n",
    "            list_key = ['train_lab', 'train_feat', 'train_id', 'devel_lab', 'devel_feat', 'devel_id', 'test_lab', 'test_feat' , 'test_id']\n",
    "            dict_feature = dict(zip(list_key, list_value))\n",
    "        for key, item in dict_feature.items():\n",
    "            item = np.asarray(item)\n",
    "            dict_feature[key] = np.concatenate(item, axis =0)\n",
    "    return dict_feature\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 19.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " egemaps: arousal\n",
      "\n",
      " Preparing Partitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "295it [00:13, 22.29it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_feature = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137646, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_feature['train_lab'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = np.concatenate(train_feat, axis=0)\n",
    "train_lab = np.concatenate(train_lab, axis=0)\n",
    "train_id = np.concatenate(train_id, axis=0)\n",
    "devel_lab = np.concatenate(devel_lab, axis=0)\n",
    "devel_feat = np.concatenate(devel_feat, axis=0)\n",
    "devel_id = np.concatenate(devel_id, axis=0)\n",
    "test_lab = np.concatenate(test_lab, axis=0)\n",
    "test_feat = np.concatenate(test_feat, axis=0)\n",
    "test_id = np.concatenate(test_id, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muse_Dataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        super().__init__()\n",
    "        self.features = torch.from_numpy(features)\n",
    "        self.labels = torch.from_numpy(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "num_classes = 3\n",
    "device = \"cuda:0\"\n",
    "feature_set = 'egemaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Muse_Dataset(train_feat, train_lab)\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True)\n",
    "\n",
    "valid_dataset = Muse_Dataset(devel_feat, devel_lab)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wave_Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n",
    "        super(Wave_Block, self).__init__()\n",
    "        self.num_rates = dilation_rates\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "\n",
    "        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n",
    "        dilation_rates = [2 ** i for i in range(dilation_rates)]\n",
    "        for dilation_rate in dilation_rates:\n",
    "            self.filter_convs.append(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n",
    "            self.gate_convs.append(\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate))\n",
    "            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convs[0](x)\n",
    "        res = x\n",
    "        for i in range(self.num_rates):\n",
    "            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n",
    "            x = self.convs[i + 1](x)\n",
    "            res = res + x\n",
    "        return res\n",
    "# detail \n",
    "class wavenet(nn.Module):\n",
    "    def __init__(self, inch=16, kernel_size=3, num_classes =3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.wave_block1 = Wave_Block(inch, 16, 4, kernel_size)\n",
    "        self.wave_block2 = Wave_Block(16, 32, 4, kernel_size)\n",
    "        self.wave_block3 = Wave_Block(32, 64, 2, kernel_size)\n",
    "        self.wave_block4 = Wave_Block(64, 128, 1, kernel_size)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.wave_block1(x)\n",
    "        x = self.wave_block2(x)\n",
    "        x = self.wave_block3(x)\n",
    "        x = self.wave_block4(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wavenet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    return round(f1_score(y_true, y_pred, average='micro') * 100, 3)\n",
    "\n",
    "\n",
    "def uar(y_true, y_pred):\n",
    "    return round(recall_score(y_true, y_pred, average='macro') * 100, 3)\n",
    "\n",
    "\n",
    "def eval_metric(predicts, targets, partition_name):\n",
    "    results = {}\n",
    "    results['f1'] = f1(targets, predicts)\n",
    "    results['uar'] = uar(targets, predicts)\n",
    "    results['combine'] = round((0.66 * results['f1'] + 0.34 * results['uar']), 3)\n",
    "    print(f'Results in {partition_name}:\\n')\n",
    "    print(\"  - f1: \", results['f1'])\n",
    "    print(\"  - uar: \", results['uar'])\n",
    "    print(\"  - combined:\", results['combine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/538 | Cost: 4.9995\n",
      "Epoch: 001/010 | Batch 020/538 | Cost: 1.4185\n",
      "Epoch: 001/010 | Batch 040/538 | Cost: 1.1394\n",
      "Epoch: 001/010 | Batch 060/538 | Cost: 1.1009\n",
      "Epoch: 001/010 | Batch 080/538 | Cost: 1.1289\n",
      "Epoch: 001/010 | Batch 100/538 | Cost: 1.1450\n",
      "Epoch: 001/010 | Batch 120/538 | Cost: 1.2835\n",
      "Epoch: 001/010 | Batch 140/538 | Cost: 1.1172\n",
      "Epoch: 001/010 | Batch 160/538 | Cost: 1.0844\n",
      "Epoch: 001/010 | Batch 180/538 | Cost: 1.1102\n",
      "Epoch: 001/010 | Batch 200/538 | Cost: 1.1039\n",
      "Epoch: 001/010 | Batch 220/538 | Cost: 1.1172\n",
      "Epoch: 001/010 | Batch 240/538 | Cost: 1.1284\n",
      "Epoch: 001/010 | Batch 260/538 | Cost: 1.1183\n",
      "Epoch: 001/010 | Batch 280/538 | Cost: 1.2351\n",
      "Epoch: 001/010 | Batch 300/538 | Cost: 1.1250\n",
      "Epoch: 001/010 | Batch 320/538 | Cost: 1.2180\n",
      "Epoch: 001/010 | Batch 340/538 | Cost: 1.1065\n",
      "Epoch: 001/010 | Batch 360/538 | Cost: 1.3170\n",
      "Epoch: 001/010 | Batch 380/538 | Cost: 1.0850\n",
      "Epoch: 001/010 | Batch 400/538 | Cost: 1.0982\n",
      "Epoch: 001/010 | Batch 420/538 | Cost: 1.1186\n",
      "Epoch: 001/010 | Batch 440/538 | Cost: 1.0972\n",
      "Epoch: 001/010 | Batch 460/538 | Cost: 1.1006\n",
      "Epoch: 001/010 | Batch 480/538 | Cost: 1.0929\n",
      "Epoch: 001/010 | Batch 500/538 | Cost: 1.1398\n",
      "Epoch: 001/010 | Batch 520/538 | Cost: 1.0993\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  34.512\n",
      "  - uar:  33.684\n",
      "  - combined: 34.23\n",
      "Time elapsed: 0.91 min\n",
      "Epoch: 002/010 | Batch 000/538 | Cost: 1.1782\n",
      "Epoch: 002/010 | Batch 020/538 | Cost: 1.1213\n",
      "Epoch: 002/010 | Batch 040/538 | Cost: 1.1270\n",
      "Epoch: 002/010 | Batch 060/538 | Cost: 1.0984\n",
      "Epoch: 002/010 | Batch 080/538 | Cost: 1.1163\n",
      "Epoch: 002/010 | Batch 100/538 | Cost: 1.1166\n",
      "Epoch: 002/010 | Batch 120/538 | Cost: 1.3620\n",
      "Epoch: 002/010 | Batch 140/538 | Cost: 1.0902\n",
      "Epoch: 002/010 | Batch 160/538 | Cost: 1.1093\n",
      "Epoch: 002/010 | Batch 180/538 | Cost: 1.0942\n",
      "Epoch: 002/010 | Batch 200/538 | Cost: 1.0776\n",
      "Epoch: 002/010 | Batch 220/538 | Cost: 1.0876\n",
      "Epoch: 002/010 | Batch 240/538 | Cost: 1.0943\n",
      "Epoch: 002/010 | Batch 260/538 | Cost: 1.0915\n",
      "Epoch: 002/010 | Batch 280/538 | Cost: 1.1061\n",
      "Epoch: 002/010 | Batch 300/538 | Cost: 1.1114\n",
      "Epoch: 002/010 | Batch 320/538 | Cost: 1.1218\n",
      "Epoch: 002/010 | Batch 340/538 | Cost: 1.0986\n",
      "Epoch: 002/010 | Batch 360/538 | Cost: 1.0969\n",
      "Epoch: 002/010 | Batch 380/538 | Cost: 1.0724\n",
      "Epoch: 002/010 | Batch 400/538 | Cost: 1.0841\n",
      "Epoch: 002/010 | Batch 420/538 | Cost: 1.0932\n",
      "Epoch: 002/010 | Batch 440/538 | Cost: 1.0973\n",
      "Epoch: 002/010 | Batch 460/538 | Cost: 1.0901\n",
      "Epoch: 002/010 | Batch 480/538 | Cost: 1.1066\n",
      "Epoch: 002/010 | Batch 500/538 | Cost: 1.0997\n",
      "Epoch: 002/010 | Batch 520/538 | Cost: 1.1397\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  34.263\n",
      "  - uar:  33.602\n",
      "  - combined: 34.038\n",
      "Time elapsed: 1.80 min\n",
      "Epoch: 003/010 | Batch 000/538 | Cost: 1.1509\n",
      "Epoch: 003/010 | Batch 020/538 | Cost: 1.0692\n",
      "Epoch: 003/010 | Batch 040/538 | Cost: 1.1320\n",
      "Epoch: 003/010 | Batch 060/538 | Cost: 1.1162\n",
      "Epoch: 003/010 | Batch 080/538 | Cost: 1.1247\n",
      "Epoch: 003/010 | Batch 100/538 | Cost: 1.0715\n",
      "Epoch: 003/010 | Batch 120/538 | Cost: 1.0891\n",
      "Epoch: 003/010 | Batch 140/538 | Cost: 1.1426\n",
      "Epoch: 003/010 | Batch 160/538 | Cost: 1.1047\n",
      "Epoch: 003/010 | Batch 180/538 | Cost: 1.1436\n",
      "Epoch: 003/010 | Batch 200/538 | Cost: 1.1165\n",
      "Epoch: 003/010 | Batch 220/538 | Cost: 1.3764\n",
      "Epoch: 003/010 | Batch 240/538 | Cost: 1.0885\n",
      "Epoch: 003/010 | Batch 260/538 | Cost: 1.1306\n",
      "Epoch: 003/010 | Batch 280/538 | Cost: 1.1063\n",
      "Epoch: 003/010 | Batch 300/538 | Cost: 1.1451\n",
      "Epoch: 003/010 | Batch 320/538 | Cost: 1.0863\n",
      "Epoch: 003/010 | Batch 340/538 | Cost: 1.1003\n",
      "Epoch: 003/010 | Batch 360/538 | Cost: 1.0772\n",
      "Epoch: 003/010 | Batch 380/538 | Cost: 1.1008\n",
      "Epoch: 003/010 | Batch 400/538 | Cost: 1.0953\n",
      "Epoch: 003/010 | Batch 420/538 | Cost: 1.1814\n",
      "Epoch: 003/010 | Batch 440/538 | Cost: 1.0861\n",
      "Epoch: 003/010 | Batch 460/538 | Cost: 1.0788\n",
      "Epoch: 003/010 | Batch 480/538 | Cost: 1.1646\n",
      "Epoch: 003/010 | Batch 500/538 | Cost: 1.0787\n",
      "Epoch: 003/010 | Batch 520/538 | Cost: 1.0662\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  33.004\n",
      "  - uar:  32.382\n",
      "  - combined: 32.793\n",
      "Time elapsed: 2.68 min\n",
      "Epoch: 004/010 | Batch 000/538 | Cost: 1.1699\n",
      "Epoch: 004/010 | Batch 020/538 | Cost: 1.0861\n",
      "Epoch: 004/010 | Batch 040/538 | Cost: 1.0848\n",
      "Epoch: 004/010 | Batch 060/538 | Cost: 1.1352\n",
      "Epoch: 004/010 | Batch 080/538 | Cost: 1.1039\n",
      "Epoch: 004/010 | Batch 100/538 | Cost: 1.0595\n",
      "Epoch: 004/010 | Batch 120/538 | Cost: 1.0922\n",
      "Epoch: 004/010 | Batch 140/538 | Cost: 1.0936\n",
      "Epoch: 004/010 | Batch 160/538 | Cost: 1.0863\n",
      "Epoch: 004/010 | Batch 180/538 | Cost: 1.0895\n",
      "Epoch: 004/010 | Batch 200/538 | Cost: 1.0892\n",
      "Epoch: 004/010 | Batch 220/538 | Cost: 1.0862\n",
      "Epoch: 004/010 | Batch 240/538 | Cost: 1.0920\n",
      "Epoch: 004/010 | Batch 260/538 | Cost: 1.0537\n",
      "Epoch: 004/010 | Batch 280/538 | Cost: 1.0694\n",
      "Epoch: 004/010 | Batch 300/538 | Cost: 1.0846\n",
      "Epoch: 004/010 | Batch 320/538 | Cost: 1.0795\n",
      "Epoch: 004/010 | Batch 340/538 | Cost: 1.0442\n",
      "Epoch: 004/010 | Batch 360/538 | Cost: 1.0947\n",
      "Epoch: 004/010 | Batch 380/538 | Cost: 1.0866\n",
      "Epoch: 004/010 | Batch 400/538 | Cost: 1.0506\n",
      "Epoch: 004/010 | Batch 420/538 | Cost: 1.0851\n",
      "Epoch: 004/010 | Batch 440/538 | Cost: 1.1036\n",
      "Epoch: 004/010 | Batch 460/538 | Cost: 1.1008\n",
      "Epoch: 004/010 | Batch 480/538 | Cost: 1.2116\n",
      "Epoch: 004/010 | Batch 500/538 | Cost: 1.1699\n",
      "Epoch: 004/010 | Batch 520/538 | Cost: 1.1702\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.1\n",
      "  - uar:  35.777\n",
      "  - combined: 35.99\n",
      "Time elapsed: 3.57 min\n",
      "Epoch: 005/010 | Batch 000/538 | Cost: 1.0965\n",
      "Epoch: 005/010 | Batch 020/538 | Cost: 1.1090\n",
      "Epoch: 005/010 | Batch 040/538 | Cost: 1.0905\n",
      "Epoch: 005/010 | Batch 060/538 | Cost: 1.0675\n",
      "Epoch: 005/010 | Batch 080/538 | Cost: 1.0611\n",
      "Epoch: 005/010 | Batch 100/538 | Cost: 1.0701\n",
      "Epoch: 005/010 | Batch 120/538 | Cost: 1.1109\n",
      "Epoch: 005/010 | Batch 140/538 | Cost: 1.0949\n",
      "Epoch: 005/010 | Batch 160/538 | Cost: 1.1372\n",
      "Epoch: 005/010 | Batch 180/538 | Cost: 1.0798\n",
      "Epoch: 005/010 | Batch 200/538 | Cost: 1.0745\n",
      "Epoch: 005/010 | Batch 220/538 | Cost: 1.1204\n",
      "Epoch: 005/010 | Batch 240/538 | Cost: 1.1588\n",
      "Epoch: 005/010 | Batch 260/538 | Cost: 1.7657\n",
      "Epoch: 005/010 | Batch 280/538 | Cost: 1.0798\n",
      "Epoch: 005/010 | Batch 300/538 | Cost: 1.0964\n",
      "Epoch: 005/010 | Batch 320/538 | Cost: 1.1080\n",
      "Epoch: 005/010 | Batch 340/538 | Cost: 1.0935\n",
      "Epoch: 005/010 | Batch 360/538 | Cost: 1.0907\n",
      "Epoch: 005/010 | Batch 380/538 | Cost: 1.1054\n",
      "Epoch: 005/010 | Batch 400/538 | Cost: 1.1640\n",
      "Epoch: 005/010 | Batch 420/538 | Cost: 1.1066\n",
      "Epoch: 005/010 | Batch 440/538 | Cost: 1.0695\n",
      "Epoch: 005/010 | Batch 460/538 | Cost: 1.0909\n",
      "Epoch: 005/010 | Batch 480/538 | Cost: 1.2821\n",
      "Epoch: 005/010 | Batch 500/538 | Cost: 1.1051\n",
      "Epoch: 005/010 | Batch 520/538 | Cost: 1.1074\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  35.57\n",
      "  - uar:  36.418\n",
      "  - combined: 35.858\n",
      "Time elapsed: 4.46 min\n",
      "Epoch: 006/010 | Batch 000/538 | Cost: 1.1504\n",
      "Epoch: 006/010 | Batch 020/538 | Cost: 1.0959\n",
      "Epoch: 006/010 | Batch 040/538 | Cost: 1.0741\n",
      "Epoch: 006/010 | Batch 060/538 | Cost: 1.0765\n",
      "Epoch: 006/010 | Batch 080/538 | Cost: 1.0735\n",
      "Epoch: 006/010 | Batch 100/538 | Cost: 1.0430\n",
      "Epoch: 006/010 | Batch 120/538 | Cost: 1.0909\n",
      "Epoch: 006/010 | Batch 140/538 | Cost: 1.0739\n",
      "Epoch: 006/010 | Batch 160/538 | Cost: 1.0699\n",
      "Epoch: 006/010 | Batch 180/538 | Cost: 1.0684\n",
      "Epoch: 006/010 | Batch 200/538 | Cost: 1.0748\n",
      "Epoch: 006/010 | Batch 220/538 | Cost: 1.0626\n",
      "Epoch: 006/010 | Batch 240/538 | Cost: 1.0573\n",
      "Epoch: 006/010 | Batch 260/538 | Cost: 1.0670\n",
      "Epoch: 006/010 | Batch 280/538 | Cost: 1.0635\n",
      "Epoch: 006/010 | Batch 300/538 | Cost: 1.0749\n",
      "Epoch: 006/010 | Batch 320/538 | Cost: 1.0435\n",
      "Epoch: 006/010 | Batch 340/538 | Cost: 1.0896\n",
      "Epoch: 006/010 | Batch 360/538 | Cost: 1.0627\n",
      "Epoch: 006/010 | Batch 380/538 | Cost: 1.0768\n",
      "Epoch: 006/010 | Batch 400/538 | Cost: 1.0462\n",
      "Epoch: 006/010 | Batch 420/538 | Cost: 1.0718\n",
      "Epoch: 006/010 | Batch 440/538 | Cost: 1.0901\n",
      "Epoch: 006/010 | Batch 460/538 | Cost: 1.0682\n",
      "Epoch: 006/010 | Batch 480/538 | Cost: 1.0537\n",
      "Epoch: 006/010 | Batch 500/538 | Cost: 1.0743\n",
      "Epoch: 006/010 | Batch 520/538 | Cost: 1.0912\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  35.052\n",
      "  - uar:  35.019\n",
      "  - combined: 35.041\n",
      "Time elapsed: 5.35 min\n",
      "Epoch: 007/010 | Batch 000/538 | Cost: 1.3121\n",
      "Epoch: 007/010 | Batch 020/538 | Cost: 1.0949\n",
      "Epoch: 007/010 | Batch 040/538 | Cost: 1.0746\n",
      "Epoch: 007/010 | Batch 060/538 | Cost: 1.0972\n",
      "Epoch: 007/010 | Batch 080/538 | Cost: 1.0764\n",
      "Epoch: 007/010 | Batch 100/538 | Cost: 1.0737\n",
      "Epoch: 007/010 | Batch 120/538 | Cost: 1.0711\n",
      "Epoch: 007/010 | Batch 140/538 | Cost: 1.0795\n",
      "Epoch: 007/010 | Batch 160/538 | Cost: 1.1168\n",
      "Epoch: 007/010 | Batch 180/538 | Cost: 1.0446\n",
      "Epoch: 007/010 | Batch 200/538 | Cost: 1.0439\n",
      "Epoch: 007/010 | Batch 220/538 | Cost: 1.0437\n",
      "Epoch: 007/010 | Batch 240/538 | Cost: 1.0791\n",
      "Epoch: 007/010 | Batch 260/538 | Cost: 1.0863\n",
      "Epoch: 007/010 | Batch 280/538 | Cost: 1.0701\n",
      "Epoch: 007/010 | Batch 300/538 | Cost: 1.0412\n",
      "Epoch: 007/010 | Batch 320/538 | Cost: 1.0951\n",
      "Epoch: 007/010 | Batch 340/538 | Cost: 1.0893\n",
      "Epoch: 007/010 | Batch 360/538 | Cost: 1.0365\n",
      "Epoch: 007/010 | Batch 380/538 | Cost: 1.0692\n",
      "Epoch: 007/010 | Batch 400/538 | Cost: 1.0954\n",
      "Epoch: 007/010 | Batch 420/538 | Cost: 1.0445\n",
      "Epoch: 007/010 | Batch 440/538 | Cost: 1.1079\n",
      "Epoch: 007/010 | Batch 460/538 | Cost: 1.0746\n",
      "Epoch: 007/010 | Batch 480/538 | Cost: 1.0516\n",
      "Epoch: 007/010 | Batch 500/538 | Cost: 1.1139\n",
      "Epoch: 007/010 | Batch 520/538 | Cost: 1.0515\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  37.846\n",
      "  - uar:  37.78\n",
      "  - combined: 37.824\n",
      "Time elapsed: 6.24 min\n",
      "Epoch: 008/010 | Batch 000/538 | Cost: 1.0597\n",
      "Epoch: 008/010 | Batch 020/538 | Cost: 1.0855\n",
      "Epoch: 008/010 | Batch 040/538 | Cost: 1.0362\n",
      "Epoch: 008/010 | Batch 060/538 | Cost: 1.0837\n",
      "Epoch: 008/010 | Batch 080/538 | Cost: 1.0211\n",
      "Epoch: 008/010 | Batch 100/538 | Cost: 1.0374\n",
      "Epoch: 008/010 | Batch 120/538 | Cost: 1.0574\n",
      "Epoch: 008/010 | Batch 140/538 | Cost: 1.0361\n",
      "Epoch: 008/010 | Batch 160/538 | Cost: 1.0591\n",
      "Epoch: 008/010 | Batch 180/538 | Cost: 1.0901\n",
      "Epoch: 008/010 | Batch 200/538 | Cost: 1.0638\n",
      "Epoch: 008/010 | Batch 220/538 | Cost: 1.1354\n",
      "Epoch: 008/010 | Batch 240/538 | Cost: 1.0703\n",
      "Epoch: 008/010 | Batch 260/538 | Cost: 1.0801\n",
      "Epoch: 008/010 | Batch 280/538 | Cost: 1.0476\n",
      "Epoch: 008/010 | Batch 300/538 | Cost: 2.0358\n",
      "Epoch: 008/010 | Batch 320/538 | Cost: 1.1747\n",
      "Epoch: 008/010 | Batch 340/538 | Cost: 1.1813\n",
      "Epoch: 008/010 | Batch 360/538 | Cost: 1.0884\n",
      "Epoch: 008/010 | Batch 380/538 | Cost: 1.1091\n",
      "Epoch: 008/010 | Batch 400/538 | Cost: 1.0336\n",
      "Epoch: 008/010 | Batch 420/538 | Cost: 1.0564\n",
      "Epoch: 008/010 | Batch 440/538 | Cost: 1.0565\n",
      "Epoch: 008/010 | Batch 460/538 | Cost: 1.1210\n",
      "Epoch: 008/010 | Batch 480/538 | Cost: 1.0570\n",
      "Epoch: 008/010 | Batch 500/538 | Cost: 1.0634\n",
      "Epoch: 008/010 | Batch 520/538 | Cost: 1.0753\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  37.016\n",
      "  - uar:  37.906\n",
      "  - combined: 37.319\n",
      "Time elapsed: 7.13 min\n",
      "Epoch: 009/010 | Batch 000/538 | Cost: 1.0834\n",
      "Epoch: 009/010 | Batch 020/538 | Cost: 1.0114\n",
      "Epoch: 009/010 | Batch 040/538 | Cost: 1.0605\n",
      "Epoch: 009/010 | Batch 060/538 | Cost: 1.0623\n",
      "Epoch: 009/010 | Batch 080/538 | Cost: 1.0766\n",
      "Epoch: 009/010 | Batch 100/538 | Cost: 1.0571\n",
      "Epoch: 009/010 | Batch 120/538 | Cost: 1.0485\n",
      "Epoch: 009/010 | Batch 140/538 | Cost: 1.0824\n",
      "Epoch: 009/010 | Batch 160/538 | Cost: 1.0446\n",
      "Epoch: 009/010 | Batch 180/538 | Cost: 1.0339\n",
      "Epoch: 009/010 | Batch 200/538 | Cost: 1.0665\n",
      "Epoch: 009/010 | Batch 220/538 | Cost: 1.1446\n",
      "Epoch: 009/010 | Batch 240/538 | Cost: 1.1515\n",
      "Epoch: 009/010 | Batch 260/538 | Cost: 1.0723\n",
      "Epoch: 009/010 | Batch 280/538 | Cost: 1.3237\n",
      "Epoch: 009/010 | Batch 300/538 | Cost: 1.0328\n",
      "Epoch: 009/010 | Batch 320/538 | Cost: 1.1004\n",
      "Epoch: 009/010 | Batch 340/538 | Cost: 1.0835\n",
      "Epoch: 009/010 | Batch 360/538 | Cost: 1.0336\n",
      "Epoch: 009/010 | Batch 380/538 | Cost: 1.0374\n",
      "Epoch: 009/010 | Batch 400/538 | Cost: 1.0308\n",
      "Epoch: 009/010 | Batch 420/538 | Cost: 1.0680\n",
      "Epoch: 009/010 | Batch 440/538 | Cost: 1.0650\n",
      "Epoch: 009/010 | Batch 460/538 | Cost: 1.0268\n",
      "Epoch: 009/010 | Batch 480/538 | Cost: 1.0363\n",
      "Epoch: 009/010 | Batch 500/538 | Cost: 1.0766\n",
      "Epoch: 009/010 | Batch 520/538 | Cost: 1.0263\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.153\n",
      "  - uar:  36.689\n",
      "  - combined: 36.335\n",
      "Time elapsed: 8.02 min\n",
      "Epoch: 010/010 | Batch 000/538 | Cost: 1.0614\n",
      "Epoch: 010/010 | Batch 020/538 | Cost: 1.0225\n",
      "Epoch: 010/010 | Batch 040/538 | Cost: 1.0708\n",
      "Epoch: 010/010 | Batch 060/538 | Cost: 1.0196\n",
      "Epoch: 010/010 | Batch 080/538 | Cost: 1.0067\n",
      "Epoch: 010/010 | Batch 100/538 | Cost: 1.0361\n",
      "Epoch: 010/010 | Batch 120/538 | Cost: 1.0186\n",
      "Epoch: 010/010 | Batch 140/538 | Cost: 1.0564\n",
      "Epoch: 010/010 | Batch 160/538 | Cost: 1.0911\n",
      "Epoch: 010/010 | Batch 180/538 | Cost: 1.0187\n",
      "Epoch: 010/010 | Batch 200/538 | Cost: 1.1270\n",
      "Epoch: 010/010 | Batch 220/538 | Cost: 1.0165\n",
      "Epoch: 010/010 | Batch 240/538 | Cost: 1.0982\n",
      "Epoch: 010/010 | Batch 260/538 | Cost: 1.0423\n",
      "Epoch: 010/010 | Batch 280/538 | Cost: 1.0378\n",
      "Epoch: 010/010 | Batch 300/538 | Cost: 1.0271\n",
      "Epoch: 010/010 | Batch 320/538 | Cost: 1.0456\n",
      "Epoch: 010/010 | Batch 340/538 | Cost: 1.0587\n",
      "Epoch: 010/010 | Batch 360/538 | Cost: 1.0618\n",
      "Epoch: 010/010 | Batch 380/538 | Cost: 1.0744\n",
      "Epoch: 010/010 | Batch 400/538 | Cost: 1.0790\n",
      "Epoch: 010/010 | Batch 420/538 | Cost: 1.0568\n",
      "Epoch: 010/010 | Batch 440/538 | Cost: 1.0426\n",
      "Epoch: 010/010 | Batch 460/538 | Cost: 1.0483\n",
      "Epoch: 010/010 | Batch 480/538 | Cost: 1.0451\n",
      "Epoch: 010/010 | Batch 500/538 | Cost: 1.0326\n",
      "Epoch: 010/010 | Batch 520/538 | Cost: 1.0872\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  35.532\n",
      "  - uar:  36.481\n",
      "  - combined: 35.855\n",
      "Time elapsed: 8.91 min\n",
      "Total Training Time: 8.91 min\n",
      "Epoch: 011/020 | Batch 000/538 | Cost: 1.2425\n",
      "Epoch: 011/020 | Batch 020/538 | Cost: 1.0080\n",
      "Epoch: 011/020 | Batch 040/538 | Cost: 1.0309\n",
      "Epoch: 011/020 | Batch 060/538 | Cost: 1.0371\n",
      "Epoch: 011/020 | Batch 080/538 | Cost: 1.0020\n",
      "Epoch: 011/020 | Batch 100/538 | Cost: 1.0345\n",
      "Epoch: 011/020 | Batch 120/538 | Cost: 1.0641\n",
      "Epoch: 011/020 | Batch 140/538 | Cost: 1.0654\n",
      "Epoch: 011/020 | Batch 160/538 | Cost: 1.0501\n",
      "Epoch: 011/020 | Batch 180/538 | Cost: 1.0276\n",
      "Epoch: 011/020 | Batch 200/538 | Cost: 1.0733\n",
      "Epoch: 011/020 | Batch 220/538 | Cost: 1.0781\n",
      "Epoch: 011/020 | Batch 240/538 | Cost: 1.1054\n",
      "Epoch: 011/020 | Batch 260/538 | Cost: 1.0176\n",
      "Epoch: 011/020 | Batch 280/538 | Cost: 1.0596\n",
      "Epoch: 011/020 | Batch 300/538 | Cost: 1.0405\n",
      "Epoch: 011/020 | Batch 320/538 | Cost: 1.0226\n",
      "Epoch: 011/020 | Batch 340/538 | Cost: 1.0105\n",
      "Epoch: 011/020 | Batch 360/538 | Cost: 1.0924\n",
      "Epoch: 011/020 | Batch 380/538 | Cost: 1.0137\n",
      "Epoch: 011/020 | Batch 400/538 | Cost: 1.0019\n",
      "Epoch: 011/020 | Batch 420/538 | Cost: 1.0446\n",
      "Epoch: 011/020 | Batch 440/538 | Cost: 1.0545\n",
      "Epoch: 011/020 | Batch 460/538 | Cost: 1.1097\n",
      "Epoch: 011/020 | Batch 480/538 | Cost: 1.0539\n",
      "Epoch: 011/020 | Batch 500/538 | Cost: 1.0346\n",
      "Epoch: 011/020 | Batch 520/538 | Cost: 1.0395\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  37.587\n",
      "  - uar:  37.831\n",
      "  - combined: 37.67\n",
      "Time elapsed: 9.80 min\n",
      "Epoch: 012/020 | Batch 000/538 | Cost: 1.0430\n",
      "Epoch: 012/020 | Batch 020/538 | Cost: 1.0168\n",
      "Epoch: 012/020 | Batch 040/538 | Cost: 0.9939\n",
      "Epoch: 012/020 | Batch 060/538 | Cost: 1.0633\n",
      "Epoch: 012/020 | Batch 080/538 | Cost: 1.0732\n",
      "Epoch: 012/020 | Batch 100/538 | Cost: 1.0232\n",
      "Epoch: 012/020 | Batch 120/538 | Cost: 1.0224\n",
      "Epoch: 012/020 | Batch 140/538 | Cost: 1.0178\n",
      "Epoch: 012/020 | Batch 160/538 | Cost: 1.0064\n",
      "Epoch: 012/020 | Batch 180/538 | Cost: 1.0198\n",
      "Epoch: 012/020 | Batch 200/538 | Cost: 1.0083\n",
      "Epoch: 012/020 | Batch 220/538 | Cost: 1.0108\n",
      "Epoch: 012/020 | Batch 240/538 | Cost: 1.0294\n",
      "Epoch: 012/020 | Batch 260/538 | Cost: 1.0358\n",
      "Epoch: 012/020 | Batch 280/538 | Cost: 1.0035\n",
      "Epoch: 012/020 | Batch 300/538 | Cost: 1.0378\n",
      "Epoch: 012/020 | Batch 320/538 | Cost: 1.0201\n",
      "Epoch: 012/020 | Batch 340/538 | Cost: 1.0360\n",
      "Epoch: 012/020 | Batch 360/538 | Cost: 1.0260\n",
      "Epoch: 012/020 | Batch 380/538 | Cost: 1.0490\n",
      "Epoch: 012/020 | Batch 400/538 | Cost: 1.1068\n",
      "Epoch: 012/020 | Batch 420/538 | Cost: 1.0473\n",
      "Epoch: 012/020 | Batch 440/538 | Cost: 1.0505\n",
      "Epoch: 012/020 | Batch 460/538 | Cost: 1.0080\n",
      "Epoch: 012/020 | Batch 480/538 | Cost: 1.1254\n",
      "Epoch: 012/020 | Batch 500/538 | Cost: 1.0574\n",
      "Epoch: 012/020 | Batch 520/538 | Cost: 1.0281\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.971\n",
      "  - uar:  37.522\n",
      "  - combined: 37.158\n",
      "Time elapsed: 10.69 min\n",
      "Epoch: 013/020 | Batch 000/538 | Cost: 0.9962\n",
      "Epoch: 013/020 | Batch 020/538 | Cost: 1.0391\n",
      "Epoch: 013/020 | Batch 040/538 | Cost: 1.0067\n",
      "Epoch: 013/020 | Batch 060/538 | Cost: 1.0436\n",
      "Epoch: 013/020 | Batch 080/538 | Cost: 1.0042\n",
      "Epoch: 013/020 | Batch 100/538 | Cost: 1.0293\n",
      "Epoch: 013/020 | Batch 120/538 | Cost: 1.0243\n",
      "Epoch: 013/020 | Batch 140/538 | Cost: 0.9901\n",
      "Epoch: 013/020 | Batch 160/538 | Cost: 1.0567\n",
      "Epoch: 013/020 | Batch 180/538 | Cost: 1.0294\n",
      "Epoch: 013/020 | Batch 200/538 | Cost: 1.0553\n",
      "Epoch: 013/020 | Batch 220/538 | Cost: 1.0379\n",
      "Epoch: 013/020 | Batch 240/538 | Cost: 1.0716\n",
      "Epoch: 013/020 | Batch 260/538 | Cost: 1.0989\n",
      "Epoch: 013/020 | Batch 280/538 | Cost: 1.0184\n",
      "Epoch: 013/020 | Batch 300/538 | Cost: 1.0243\n",
      "Epoch: 013/020 | Batch 320/538 | Cost: 1.0523\n",
      "Epoch: 013/020 | Batch 340/538 | Cost: 1.0223\n",
      "Epoch: 013/020 | Batch 360/538 | Cost: 1.0099\n",
      "Epoch: 013/020 | Batch 380/538 | Cost: 1.0306\n",
      "Epoch: 013/020 | Batch 400/538 | Cost: 1.0545\n",
      "Epoch: 013/020 | Batch 420/538 | Cost: 1.0770\n",
      "Epoch: 013/020 | Batch 440/538 | Cost: 1.0286\n",
      "Epoch: 013/020 | Batch 460/538 | Cost: 1.0144\n",
      "Epoch: 013/020 | Batch 480/538 | Cost: 1.0420\n",
      "Epoch: 013/020 | Batch 500/538 | Cost: 1.0253\n",
      "Epoch: 013/020 | Batch 520/538 | Cost: 1.0362\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.217\n",
      "  - uar:  36.664\n",
      "  - combined: 36.369\n",
      "Time elapsed: 11.57 min\n",
      "Epoch: 014/020 | Batch 000/538 | Cost: 1.0179\n",
      "Epoch: 014/020 | Batch 020/538 | Cost: 1.0055\n",
      "Epoch: 014/020 | Batch 040/538 | Cost: 1.0322\n",
      "Epoch: 014/020 | Batch 060/538 | Cost: 1.0102\n",
      "Epoch: 014/020 | Batch 080/538 | Cost: 1.0135\n",
      "Epoch: 014/020 | Batch 100/538 | Cost: 1.0146\n",
      "Epoch: 014/020 | Batch 120/538 | Cost: 1.0312\n",
      "Epoch: 014/020 | Batch 140/538 | Cost: 1.0357\n",
      "Epoch: 014/020 | Batch 160/538 | Cost: 1.0306\n",
      "Epoch: 014/020 | Batch 180/538 | Cost: 1.0360\n",
      "Epoch: 014/020 | Batch 200/538 | Cost: 1.0162\n",
      "Epoch: 014/020 | Batch 220/538 | Cost: 1.0267\n",
      "Epoch: 014/020 | Batch 240/538 | Cost: 1.0437\n",
      "Epoch: 014/020 | Batch 260/538 | Cost: 1.0082\n",
      "Epoch: 014/020 | Batch 280/538 | Cost: 1.0152\n",
      "Epoch: 014/020 | Batch 300/538 | Cost: 1.0164\n",
      "Epoch: 014/020 | Batch 320/538 | Cost: 1.0448\n",
      "Epoch: 014/020 | Batch 340/538 | Cost: 1.0134\n",
      "Epoch: 014/020 | Batch 360/538 | Cost: 1.0475\n",
      "Epoch: 014/020 | Batch 380/538 | Cost: 1.0391\n",
      "Epoch: 014/020 | Batch 400/538 | Cost: 1.0672\n",
      "Epoch: 014/020 | Batch 420/538 | Cost: 1.0566\n",
      "Epoch: 014/020 | Batch 440/538 | Cost: 1.0047\n",
      "Epoch: 014/020 | Batch 460/538 | Cost: 1.0251\n",
      "Epoch: 014/020 | Batch 480/538 | Cost: 1.0359\n",
      "Epoch: 014/020 | Batch 500/538 | Cost: 1.0443\n",
      "Epoch: 014/020 | Batch 520/538 | Cost: 1.0157\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  37.293\n",
      "  - uar:  37.581\n",
      "  - combined: 37.391\n",
      "Time elapsed: 12.46 min\n",
      "Epoch: 015/020 | Batch 000/538 | Cost: 1.0118\n",
      "Epoch: 015/020 | Batch 020/538 | Cost: 0.9945\n",
      "Epoch: 015/020 | Batch 040/538 | Cost: 1.0219\n",
      "Epoch: 015/020 | Batch 060/538 | Cost: 1.0186\n",
      "Epoch: 015/020 | Batch 080/538 | Cost: 1.0528\n",
      "Epoch: 015/020 | Batch 100/538 | Cost: 1.0522\n",
      "Epoch: 015/020 | Batch 120/538 | Cost: 1.0528\n",
      "Epoch: 015/020 | Batch 140/538 | Cost: 1.0115\n",
      "Epoch: 015/020 | Batch 160/538 | Cost: 0.9872\n",
      "Epoch: 015/020 | Batch 180/538 | Cost: 1.0559\n",
      "Epoch: 015/020 | Batch 200/538 | Cost: 1.0166\n",
      "Epoch: 015/020 | Batch 220/538 | Cost: 1.0945\n",
      "Epoch: 015/020 | Batch 240/538 | Cost: 1.0665\n",
      "Epoch: 015/020 | Batch 260/538 | Cost: 1.0208\n",
      "Epoch: 015/020 | Batch 280/538 | Cost: 1.0452\n",
      "Epoch: 015/020 | Batch 300/538 | Cost: 1.0550\n",
      "Epoch: 015/020 | Batch 320/538 | Cost: 1.0196\n",
      "Epoch: 015/020 | Batch 340/538 | Cost: 1.0137\n",
      "Epoch: 015/020 | Batch 360/538 | Cost: 1.0325\n",
      "Epoch: 015/020 | Batch 380/538 | Cost: 1.0588\n",
      "Epoch: 015/020 | Batch 400/538 | Cost: 1.1859\n",
      "Epoch: 015/020 | Batch 420/538 | Cost: 1.0119\n",
      "Epoch: 015/020 | Batch 440/538 | Cost: 1.0422\n",
      "Epoch: 015/020 | Batch 460/538 | Cost: 1.0484\n",
      "Epoch: 015/020 | Batch 480/538 | Cost: 1.0569\n",
      "Epoch: 015/020 | Batch 500/538 | Cost: 1.0287\n",
      "Epoch: 015/020 | Batch 520/538 | Cost: 1.0509\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.856\n",
      "  - uar:  37.199\n",
      "  - combined: 36.973\n",
      "Time elapsed: 13.35 min\n",
      "Epoch: 016/020 | Batch 000/538 | Cost: 1.0351\n",
      "Epoch: 016/020 | Batch 020/538 | Cost: 1.0077\n",
      "Epoch: 016/020 | Batch 040/538 | Cost: 1.0499\n",
      "Epoch: 016/020 | Batch 060/538 | Cost: 1.0239\n",
      "Epoch: 016/020 | Batch 080/538 | Cost: 1.0445\n",
      "Epoch: 016/020 | Batch 100/538 | Cost: 1.0083\n",
      "Epoch: 016/020 | Batch 120/538 | Cost: 0.9928\n",
      "Epoch: 016/020 | Batch 140/538 | Cost: 0.9680\n",
      "Epoch: 016/020 | Batch 160/538 | Cost: 1.0569\n",
      "Epoch: 016/020 | Batch 180/538 | Cost: 1.0366\n",
      "Epoch: 016/020 | Batch 200/538 | Cost: 1.0500\n",
      "Epoch: 016/020 | Batch 220/538 | Cost: 1.0730\n",
      "Epoch: 016/020 | Batch 240/538 | Cost: 1.0524\n",
      "Epoch: 016/020 | Batch 260/538 | Cost: 1.0058\n",
      "Epoch: 016/020 | Batch 280/538 | Cost: 1.0160\n",
      "Epoch: 016/020 | Batch 300/538 | Cost: 1.0141\n",
      "Epoch: 016/020 | Batch 320/538 | Cost: 1.0515\n",
      "Epoch: 016/020 | Batch 340/538 | Cost: 1.0088\n",
      "Epoch: 016/020 | Batch 360/538 | Cost: 1.1495\n",
      "Epoch: 016/020 | Batch 380/538 | Cost: 1.0090\n",
      "Epoch: 016/020 | Batch 400/538 | Cost: 1.0171\n",
      "Epoch: 016/020 | Batch 420/538 | Cost: 1.0325\n",
      "Epoch: 016/020 | Batch 440/538 | Cost: 1.0730\n",
      "Epoch: 016/020 | Batch 460/538 | Cost: 1.0363\n",
      "Epoch: 016/020 | Batch 480/538 | Cost: 1.0398\n",
      "Epoch: 016/020 | Batch 500/538 | Cost: 1.0106\n",
      "Epoch: 016/020 | Batch 520/538 | Cost: 1.0324\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  37.128\n",
      "  - uar:  37.596\n",
      "  - combined: 37.287\n",
      "Time elapsed: 14.24 min\n",
      "Epoch: 017/020 | Batch 000/538 | Cost: 1.0454\n",
      "Epoch: 017/020 | Batch 020/538 | Cost: 1.0420\n",
      "Epoch: 017/020 | Batch 040/538 | Cost: 1.0199\n",
      "Epoch: 017/020 | Batch 060/538 | Cost: 1.0700\n",
      "Epoch: 017/020 | Batch 080/538 | Cost: 1.0047\n",
      "Epoch: 017/020 | Batch 100/538 | Cost: 1.0074\n",
      "Epoch: 017/020 | Batch 120/538 | Cost: 1.0566\n",
      "Epoch: 017/020 | Batch 140/538 | Cost: 1.0278\n",
      "Epoch: 017/020 | Batch 160/538 | Cost: 1.0967\n",
      "Epoch: 017/020 | Batch 180/538 | Cost: 1.0481\n",
      "Epoch: 017/020 | Batch 200/538 | Cost: 0.9870\n",
      "Epoch: 017/020 | Batch 220/538 | Cost: 1.0110\n",
      "Epoch: 017/020 | Batch 240/538 | Cost: 1.0193\n",
      "Epoch: 017/020 | Batch 260/538 | Cost: 1.0498\n",
      "Epoch: 017/020 | Batch 280/538 | Cost: 1.0328\n",
      "Epoch: 017/020 | Batch 300/538 | Cost: 1.0283\n",
      "Epoch: 017/020 | Batch 320/538 | Cost: 1.1066\n",
      "Epoch: 017/020 | Batch 340/538 | Cost: 1.0319\n",
      "Epoch: 017/020 | Batch 360/538 | Cost: 1.0410\n",
      "Epoch: 017/020 | Batch 380/538 | Cost: 1.0199\n",
      "Epoch: 017/020 | Batch 400/538 | Cost: 1.0009\n",
      "Epoch: 017/020 | Batch 420/538 | Cost: 1.0860\n",
      "Epoch: 017/020 | Batch 440/538 | Cost: 1.0885\n",
      "Epoch: 017/020 | Batch 460/538 | Cost: 0.9984\n",
      "Epoch: 017/020 | Batch 480/538 | Cost: 0.9954\n",
      "Epoch: 017/020 | Batch 500/538 | Cost: 1.0625\n",
      "Epoch: 017/020 | Batch 520/538 | Cost: 1.0148\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.217\n",
      "  - uar:  36.853\n",
      "  - combined: 36.433\n",
      "Time elapsed: 15.13 min\n",
      "Epoch: 018/020 | Batch 000/538 | Cost: 1.0102\n",
      "Epoch: 018/020 | Batch 020/538 | Cost: 1.0238\n",
      "Epoch: 018/020 | Batch 040/538 | Cost: 1.0305\n",
      "Epoch: 018/020 | Batch 060/538 | Cost: 1.0366\n",
      "Epoch: 018/020 | Batch 080/538 | Cost: 1.0581\n",
      "Epoch: 018/020 | Batch 100/538 | Cost: 1.0341\n",
      "Epoch: 018/020 | Batch 120/538 | Cost: 1.0285\n",
      "Epoch: 018/020 | Batch 140/538 | Cost: 1.0497\n",
      "Epoch: 018/020 | Batch 160/538 | Cost: 1.0162\n",
      "Epoch: 018/020 | Batch 180/538 | Cost: 1.0054\n",
      "Epoch: 018/020 | Batch 200/538 | Cost: 0.9993\n",
      "Epoch: 018/020 | Batch 220/538 | Cost: 1.0359\n",
      "Epoch: 018/020 | Batch 240/538 | Cost: 0.9996\n",
      "Epoch: 018/020 | Batch 260/538 | Cost: 1.0584\n",
      "Epoch: 018/020 | Batch 280/538 | Cost: 1.0110\n",
      "Epoch: 018/020 | Batch 300/538 | Cost: 1.0634\n",
      "Epoch: 018/020 | Batch 320/538 | Cost: 1.0235\n",
      "Epoch: 018/020 | Batch 340/538 | Cost: 0.9964\n",
      "Epoch: 018/020 | Batch 360/538 | Cost: 0.9811\n",
      "Epoch: 018/020 | Batch 380/538 | Cost: 1.0235\n",
      "Epoch: 018/020 | Batch 400/538 | Cost: 0.9900\n",
      "Epoch: 018/020 | Batch 420/538 | Cost: 1.0280\n",
      "Epoch: 018/020 | Batch 440/538 | Cost: 1.0671\n",
      "Epoch: 018/020 | Batch 460/538 | Cost: 1.0407\n",
      "Epoch: 018/020 | Batch 480/538 | Cost: 1.0722\n",
      "Epoch: 018/020 | Batch 500/538 | Cost: 1.0074\n",
      "Epoch: 018/020 | Batch 520/538 | Cost: 1.0519\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.049\n",
      "  - uar:  36.688\n",
      "  - combined: 36.266\n",
      "Time elapsed: 16.02 min\n",
      "Epoch: 019/020 | Batch 000/538 | Cost: 0.9948\n",
      "Epoch: 019/020 | Batch 020/538 | Cost: 1.0048\n",
      "Epoch: 019/020 | Batch 040/538 | Cost: 1.0255\n",
      "Epoch: 019/020 | Batch 060/538 | Cost: 1.0290\n",
      "Epoch: 019/020 | Batch 080/538 | Cost: 1.0137\n",
      "Epoch: 019/020 | Batch 100/538 | Cost: 1.0075\n",
      "Epoch: 019/020 | Batch 120/538 | Cost: 1.0497\n",
      "Epoch: 019/020 | Batch 140/538 | Cost: 1.0171\n",
      "Epoch: 019/020 | Batch 160/538 | Cost: 1.0534\n",
      "Epoch: 019/020 | Batch 180/538 | Cost: 1.0398\n",
      "Epoch: 019/020 | Batch 200/538 | Cost: 0.9754\n",
      "Epoch: 019/020 | Batch 220/538 | Cost: 1.0417\n",
      "Epoch: 019/020 | Batch 240/538 | Cost: 1.0650\n",
      "Epoch: 019/020 | Batch 260/538 | Cost: 1.0321\n",
      "Epoch: 019/020 | Batch 280/538 | Cost: 1.0551\n",
      "Epoch: 019/020 | Batch 300/538 | Cost: 1.0124\n",
      "Epoch: 019/020 | Batch 320/538 | Cost: 1.0354\n",
      "Epoch: 019/020 | Batch 340/538 | Cost: 1.0079\n",
      "Epoch: 019/020 | Batch 360/538 | Cost: 1.0023\n",
      "Epoch: 019/020 | Batch 380/538 | Cost: 1.0192\n",
      "Epoch: 019/020 | Batch 400/538 | Cost: 0.9926\n",
      "Epoch: 019/020 | Batch 420/538 | Cost: 1.0345\n",
      "Epoch: 019/020 | Batch 440/538 | Cost: 0.9800\n",
      "Epoch: 019/020 | Batch 460/538 | Cost: 1.0054\n",
      "Epoch: 019/020 | Batch 480/538 | Cost: 1.0164\n",
      "Epoch: 019/020 | Batch 500/538 | Cost: 1.0182\n",
      "Epoch: 019/020 | Batch 520/538 | Cost: 1.0449\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  35.836\n",
      "  - uar:  36.467\n",
      "  - combined: 36.051\n",
      "Time elapsed: 16.91 min\n",
      "Epoch: 020/020 | Batch 000/538 | Cost: 1.0560\n",
      "Epoch: 020/020 | Batch 020/538 | Cost: 1.0341\n",
      "Epoch: 020/020 | Batch 040/538 | Cost: 1.0220\n",
      "Epoch: 020/020 | Batch 060/538 | Cost: 0.9908\n",
      "Epoch: 020/020 | Batch 080/538 | Cost: 1.0450\n",
      "Epoch: 020/020 | Batch 100/538 | Cost: 1.0388\n",
      "Epoch: 020/020 | Batch 120/538 | Cost: 1.0733\n",
      "Epoch: 020/020 | Batch 140/538 | Cost: 1.0253\n",
      "Epoch: 020/020 | Batch 160/538 | Cost: 0.9912\n",
      "Epoch: 020/020 | Batch 180/538 | Cost: 1.0375\n",
      "Epoch: 020/020 | Batch 200/538 | Cost: 1.0031\n",
      "Epoch: 020/020 | Batch 220/538 | Cost: 1.1072\n",
      "Epoch: 020/020 | Batch 240/538 | Cost: 1.0375\n",
      "Epoch: 020/020 | Batch 260/538 | Cost: 1.0853\n",
      "Epoch: 020/020 | Batch 280/538 | Cost: 1.0202\n",
      "Epoch: 020/020 | Batch 300/538 | Cost: 1.0555\n",
      "Epoch: 020/020 | Batch 320/538 | Cost: 1.0406\n",
      "Epoch: 020/020 | Batch 340/538 | Cost: 1.0234\n",
      "Epoch: 020/020 | Batch 360/538 | Cost: 1.0208\n",
      "Epoch: 020/020 | Batch 380/538 | Cost: 1.0197\n",
      "Epoch: 020/020 | Batch 400/538 | Cost: 1.0482\n",
      "Epoch: 020/020 | Batch 420/538 | Cost: 1.0620\n",
      "Epoch: 020/020 | Batch 440/538 | Cost: 1.0229\n",
      "Epoch: 020/020 | Batch 460/538 | Cost: 1.0415\n",
      "Epoch: 020/020 | Batch 480/538 | Cost: 1.0260\n",
      "Epoch: 020/020 | Batch 500/538 | Cost: 1.0314\n",
      "Epoch: 020/020 | Batch 520/538 | Cost: 1.0087\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  35.687\n",
      "  - uar:  36.331\n",
      "  - combined: 35.906\n",
      "Time elapsed: 17.80 min\n",
      "Total Training Time: 17.80 min\n",
      "Epoch: 021/030 | Batch 000/538 | Cost: 1.0009\n",
      "Epoch: 021/030 | Batch 020/538 | Cost: 1.0511\n",
      "Epoch: 021/030 | Batch 040/538 | Cost: 1.0249\n",
      "Epoch: 021/030 | Batch 060/538 | Cost: 1.0119\n",
      "Epoch: 021/030 | Batch 080/538 | Cost: 1.0101\n",
      "Epoch: 021/030 | Batch 100/538 | Cost: 1.0389\n",
      "Epoch: 021/030 | Batch 120/538 | Cost: 0.9956\n",
      "Epoch: 021/030 | Batch 140/538 | Cost: 1.0409\n",
      "Epoch: 021/030 | Batch 160/538 | Cost: 1.0640\n",
      "Epoch: 021/030 | Batch 180/538 | Cost: 1.0219\n",
      "Epoch: 021/030 | Batch 200/538 | Cost: 1.0126\n",
      "Epoch: 021/030 | Batch 220/538 | Cost: 1.0488\n",
      "Epoch: 021/030 | Batch 240/538 | Cost: 1.0297\n",
      "Epoch: 021/030 | Batch 260/538 | Cost: 1.0316\n",
      "Epoch: 021/030 | Batch 280/538 | Cost: 1.0167\n",
      "Epoch: 021/030 | Batch 300/538 | Cost: 1.0549\n",
      "Epoch: 021/030 | Batch 320/538 | Cost: 1.0311\n",
      "Epoch: 021/030 | Batch 340/538 | Cost: 1.0519\n",
      "Epoch: 021/030 | Batch 360/538 | Cost: 1.0319\n",
      "Epoch: 021/030 | Batch 380/538 | Cost: 1.0243\n",
      "Epoch: 021/030 | Batch 400/538 | Cost: 1.0014\n",
      "Epoch: 021/030 | Batch 420/538 | Cost: 1.0456\n",
      "Epoch: 021/030 | Batch 440/538 | Cost: 1.0068\n",
      "Epoch: 021/030 | Batch 460/538 | Cost: 1.0555\n",
      "Epoch: 021/030 | Batch 480/538 | Cost: 1.0174\n",
      "Epoch: 021/030 | Batch 500/538 | Cost: 1.0266\n",
      "Epoch: 021/030 | Batch 520/538 | Cost: 1.0494\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  35.986\n",
      "  - uar:  36.583\n",
      "  - combined: 36.189\n",
      "Time elapsed: 18.69 min\n",
      "Epoch: 022/030 | Batch 000/538 | Cost: 1.0541\n",
      "Epoch: 022/030 | Batch 020/538 | Cost: 1.0669\n",
      "Epoch: 022/030 | Batch 040/538 | Cost: 0.9964\n",
      "Epoch: 022/030 | Batch 060/538 | Cost: 1.0138\n",
      "Epoch: 022/030 | Batch 080/538 | Cost: 0.9867\n",
      "Epoch: 022/030 | Batch 100/538 | Cost: 1.0112\n",
      "Epoch: 022/030 | Batch 120/538 | Cost: 1.0180\n",
      "Epoch: 022/030 | Batch 140/538 | Cost: 1.0367\n",
      "Epoch: 022/030 | Batch 160/538 | Cost: 1.0198\n",
      "Epoch: 022/030 | Batch 180/538 | Cost: 1.0269\n",
      "Epoch: 022/030 | Batch 200/538 | Cost: 0.9764\n",
      "Epoch: 022/030 | Batch 220/538 | Cost: 1.0304\n",
      "Epoch: 022/030 | Batch 240/538 | Cost: 1.0237\n",
      "Epoch: 022/030 | Batch 260/538 | Cost: 0.9984\n",
      "Epoch: 022/030 | Batch 280/538 | Cost: 1.0234\n",
      "Epoch: 022/030 | Batch 300/538 | Cost: 0.9914\n",
      "Epoch: 022/030 | Batch 320/538 | Cost: 1.0105\n",
      "Epoch: 022/030 | Batch 340/538 | Cost: 0.9968\n",
      "Epoch: 022/030 | Batch 360/538 | Cost: 1.0687\n",
      "Epoch: 022/030 | Batch 380/538 | Cost: 1.0330\n",
      "Epoch: 022/030 | Batch 400/538 | Cost: 1.0438\n",
      "Epoch: 022/030 | Batch 420/538 | Cost: 1.0550\n",
      "Epoch: 022/030 | Batch 440/538 | Cost: 1.0204\n",
      "Epoch: 022/030 | Batch 460/538 | Cost: 1.0356\n",
      "Epoch: 022/030 | Batch 480/538 | Cost: 0.9948\n",
      "Epoch: 022/030 | Batch 500/538 | Cost: 1.0303\n",
      "Epoch: 022/030 | Batch 520/538 | Cost: 1.0498\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.356\n",
      "  - uar:  36.827\n",
      "  - combined: 36.516\n",
      "Time elapsed: 19.58 min\n",
      "Epoch: 023/030 | Batch 000/538 | Cost: 1.0007\n",
      "Epoch: 023/030 | Batch 020/538 | Cost: 1.0066\n",
      "Epoch: 023/030 | Batch 040/538 | Cost: 1.0580\n",
      "Epoch: 023/030 | Batch 060/538 | Cost: 1.0170\n",
      "Epoch: 023/030 | Batch 080/538 | Cost: 1.0088\n",
      "Epoch: 023/030 | Batch 100/538 | Cost: 0.9954\n",
      "Epoch: 023/030 | Batch 120/538 | Cost: 1.0051\n",
      "Epoch: 023/030 | Batch 140/538 | Cost: 1.0199\n",
      "Epoch: 023/030 | Batch 160/538 | Cost: 1.0268\n",
      "Epoch: 023/030 | Batch 180/538 | Cost: 0.9941\n",
      "Epoch: 023/030 | Batch 200/538 | Cost: 1.0104\n",
      "Epoch: 023/030 | Batch 220/538 | Cost: 1.0573\n",
      "Epoch: 023/030 | Batch 240/538 | Cost: 1.0767\n",
      "Epoch: 023/030 | Batch 260/538 | Cost: 1.0374\n",
      "Epoch: 023/030 | Batch 280/538 | Cost: 1.0286\n",
      "Epoch: 023/030 | Batch 300/538 | Cost: 1.0461\n",
      "Epoch: 023/030 | Batch 320/538 | Cost: 0.9984\n",
      "Epoch: 023/030 | Batch 340/538 | Cost: 1.0175\n",
      "Epoch: 023/030 | Batch 360/538 | Cost: 1.0331\n",
      "Epoch: 023/030 | Batch 380/538 | Cost: 1.0800\n",
      "Epoch: 023/030 | Batch 400/538 | Cost: 1.0410\n",
      "Epoch: 023/030 | Batch 420/538 | Cost: 1.0219\n",
      "Epoch: 023/030 | Batch 440/538 | Cost: 0.9914\n",
      "Epoch: 023/030 | Batch 460/538 | Cost: 0.9967\n",
      "Epoch: 023/030 | Batch 480/538 | Cost: 1.0209\n",
      "Epoch: 023/030 | Batch 500/538 | Cost: 0.9885\n",
      "Epoch: 023/030 | Batch 520/538 | Cost: 1.0385\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.095\n",
      "  - uar:  36.644\n",
      "  - combined: 36.282\n",
      "Time elapsed: 20.47 min\n",
      "Epoch: 024/030 | Batch 000/538 | Cost: 1.0073\n",
      "Epoch: 024/030 | Batch 020/538 | Cost: 0.9825\n",
      "Epoch: 024/030 | Batch 040/538 | Cost: 1.0286\n",
      "Epoch: 024/030 | Batch 060/538 | Cost: 1.0204\n",
      "Epoch: 024/030 | Batch 080/538 | Cost: 1.0143\n",
      "Epoch: 024/030 | Batch 100/538 | Cost: 1.0383\n",
      "Epoch: 024/030 | Batch 120/538 | Cost: 1.0143\n",
      "Epoch: 024/030 | Batch 140/538 | Cost: 1.0208\n",
      "Epoch: 024/030 | Batch 160/538 | Cost: 1.0158\n",
      "Epoch: 024/030 | Batch 180/538 | Cost: 1.0201\n",
      "Epoch: 024/030 | Batch 200/538 | Cost: 0.9857\n",
      "Epoch: 024/030 | Batch 220/538 | Cost: 1.0141\n",
      "Epoch: 024/030 | Batch 240/538 | Cost: 1.0150\n",
      "Epoch: 024/030 | Batch 260/538 | Cost: 1.0408\n",
      "Epoch: 024/030 | Batch 280/538 | Cost: 1.1043\n",
      "Epoch: 024/030 | Batch 300/538 | Cost: 1.0088\n",
      "Epoch: 024/030 | Batch 320/538 | Cost: 1.0297\n",
      "Epoch: 024/030 | Batch 340/538 | Cost: 1.0161\n",
      "Epoch: 024/030 | Batch 360/538 | Cost: 1.0012\n",
      "Epoch: 024/030 | Batch 380/538 | Cost: 1.0552\n",
      "Epoch: 024/030 | Batch 400/538 | Cost: 1.0772\n",
      "Epoch: 024/030 | Batch 420/538 | Cost: 1.0014\n",
      "Epoch: 024/030 | Batch 440/538 | Cost: 1.0171\n",
      "Epoch: 024/030 | Batch 460/538 | Cost: 1.0255\n",
      "Epoch: 024/030 | Batch 480/538 | Cost: 1.0435\n",
      "Epoch: 024/030 | Batch 500/538 | Cost: 1.0196\n",
      "Epoch: 024/030 | Batch 520/538 | Cost: 1.0655\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.1\n",
      "  - uar:  36.702\n",
      "  - combined: 36.305\n",
      "Time elapsed: 21.36 min\n",
      "Epoch: 025/030 | Batch 000/538 | Cost: 1.0256\n",
      "Epoch: 025/030 | Batch 020/538 | Cost: 1.0050\n",
      "Epoch: 025/030 | Batch 040/538 | Cost: 0.9734\n",
      "Epoch: 025/030 | Batch 060/538 | Cost: 1.0105\n",
      "Epoch: 025/030 | Batch 080/538 | Cost: 1.0165\n",
      "Epoch: 025/030 | Batch 100/538 | Cost: 1.0418\n",
      "Epoch: 025/030 | Batch 120/538 | Cost: 1.0349\n",
      "Epoch: 025/030 | Batch 140/538 | Cost: 1.0013\n",
      "Epoch: 025/030 | Batch 160/538 | Cost: 1.0053\n",
      "Epoch: 025/030 | Batch 180/538 | Cost: 0.9965\n",
      "Epoch: 025/030 | Batch 200/538 | Cost: 1.0806\n",
      "Epoch: 025/030 | Batch 220/538 | Cost: 0.9973\n",
      "Epoch: 025/030 | Batch 240/538 | Cost: 1.0445\n",
      "Epoch: 025/030 | Batch 260/538 | Cost: 0.9981\n",
      "Epoch: 025/030 | Batch 280/538 | Cost: 0.9969\n",
      "Epoch: 025/030 | Batch 300/538 | Cost: 1.0433\n",
      "Epoch: 025/030 | Batch 320/538 | Cost: 1.0388\n",
      "Epoch: 025/030 | Batch 340/538 | Cost: 1.0402\n",
      "Epoch: 025/030 | Batch 360/538 | Cost: 0.9980\n",
      "Epoch: 025/030 | Batch 380/538 | Cost: 1.0144\n",
      "Epoch: 025/030 | Batch 400/538 | Cost: 1.0019\n",
      "Epoch: 025/030 | Batch 420/538 | Cost: 0.9825\n",
      "Epoch: 025/030 | Batch 440/538 | Cost: 1.0202\n",
      "Epoch: 025/030 | Batch 460/538 | Cost: 1.0099\n",
      "Epoch: 025/030 | Batch 480/538 | Cost: 1.0211\n",
      "Epoch: 025/030 | Batch 500/538 | Cost: 1.0187\n",
      "Epoch: 025/030 | Batch 520/538 | Cost: 1.0036\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.255\n",
      "  - uar:  36.804\n",
      "  - combined: 36.442\n",
      "Time elapsed: 22.24 min\n",
      "Epoch: 026/030 | Batch 000/538 | Cost: 1.0393\n",
      "Epoch: 026/030 | Batch 020/538 | Cost: 1.0291\n",
      "Epoch: 026/030 | Batch 040/538 | Cost: 0.9708\n",
      "Epoch: 026/030 | Batch 060/538 | Cost: 1.0098\n",
      "Epoch: 026/030 | Batch 080/538 | Cost: 1.0537\n",
      "Epoch: 026/030 | Batch 100/538 | Cost: 0.9827\n",
      "Epoch: 026/030 | Batch 120/538 | Cost: 1.0556\n",
      "Epoch: 026/030 | Batch 140/538 | Cost: 1.0850\n",
      "Epoch: 026/030 | Batch 160/538 | Cost: 1.0381\n",
      "Epoch: 026/030 | Batch 180/538 | Cost: 1.0238\n",
      "Epoch: 026/030 | Batch 200/538 | Cost: 1.0246\n",
      "Epoch: 026/030 | Batch 220/538 | Cost: 1.0333\n",
      "Epoch: 026/030 | Batch 240/538 | Cost: 1.0286\n",
      "Epoch: 026/030 | Batch 260/538 | Cost: 1.0219\n",
      "Epoch: 026/030 | Batch 280/538 | Cost: 0.9979\n",
      "Epoch: 026/030 | Batch 300/538 | Cost: 0.9959\n",
      "Epoch: 026/030 | Batch 320/538 | Cost: 1.0543\n",
      "Epoch: 026/030 | Batch 340/538 | Cost: 1.0067\n",
      "Epoch: 026/030 | Batch 360/538 | Cost: 1.0058\n",
      "Epoch: 026/030 | Batch 380/538 | Cost: 1.0218\n",
      "Epoch: 026/030 | Batch 400/538 | Cost: 1.0267\n",
      "Epoch: 026/030 | Batch 420/538 | Cost: 1.0048\n",
      "Epoch: 026/030 | Batch 440/538 | Cost: 1.0314\n",
      "Epoch: 026/030 | Batch 460/538 | Cost: 1.0365\n",
      "Epoch: 026/030 | Batch 480/538 | Cost: 0.9674\n",
      "Epoch: 026/030 | Batch 500/538 | Cost: 1.0532\n",
      "Epoch: 026/030 | Batch 520/538 | Cost: 1.0196\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.105\n",
      "  - uar:  36.669\n",
      "  - combined: 36.297\n",
      "Time elapsed: 23.13 min\n",
      "Epoch: 027/030 | Batch 000/538 | Cost: 1.0265\n",
      "Epoch: 027/030 | Batch 020/538 | Cost: 1.0166\n",
      "Epoch: 027/030 | Batch 040/538 | Cost: 1.0339\n",
      "Epoch: 027/030 | Batch 060/538 | Cost: 0.9935\n",
      "Epoch: 027/030 | Batch 080/538 | Cost: 1.0204\n",
      "Epoch: 027/030 | Batch 100/538 | Cost: 1.0702\n",
      "Epoch: 027/030 | Batch 120/538 | Cost: 1.0417\n",
      "Epoch: 027/030 | Batch 140/538 | Cost: 1.0423\n",
      "Epoch: 027/030 | Batch 160/538 | Cost: 1.0485\n",
      "Epoch: 027/030 | Batch 180/538 | Cost: 1.0433\n",
      "Epoch: 027/030 | Batch 200/538 | Cost: 1.0099\n",
      "Epoch: 027/030 | Batch 220/538 | Cost: 1.0215\n",
      "Epoch: 027/030 | Batch 240/538 | Cost: 1.0069\n",
      "Epoch: 027/030 | Batch 260/538 | Cost: 1.0613\n",
      "Epoch: 027/030 | Batch 280/538 | Cost: 1.0224\n",
      "Epoch: 027/030 | Batch 300/538 | Cost: 1.0424\n",
      "Epoch: 027/030 | Batch 320/538 | Cost: 0.9917\n",
      "Epoch: 027/030 | Batch 340/538 | Cost: 1.0374\n",
      "Epoch: 027/030 | Batch 360/538 | Cost: 1.0394\n",
      "Epoch: 027/030 | Batch 380/538 | Cost: 1.0536\n",
      "Epoch: 027/030 | Batch 400/538 | Cost: 1.0151\n",
      "Epoch: 027/030 | Batch 420/538 | Cost: 1.0029\n",
      "Epoch: 027/030 | Batch 440/538 | Cost: 1.0103\n",
      "Epoch: 027/030 | Batch 460/538 | Cost: 1.0132\n",
      "Epoch: 027/030 | Batch 480/538 | Cost: 1.0078\n",
      "Epoch: 027/030 | Batch 500/538 | Cost: 1.0446\n",
      "Epoch: 027/030 | Batch 520/538 | Cost: 1.0299\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.28\n",
      "  - uar:  36.804\n",
      "  - combined: 36.458\n",
      "Time elapsed: 24.02 min\n",
      "Epoch: 028/030 | Batch 000/538 | Cost: 1.0432\n",
      "Epoch: 028/030 | Batch 020/538 | Cost: 1.0128\n",
      "Epoch: 028/030 | Batch 040/538 | Cost: 1.0060\n",
      "Epoch: 028/030 | Batch 060/538 | Cost: 1.0351\n",
      "Epoch: 028/030 | Batch 080/538 | Cost: 1.0391\n",
      "Epoch: 028/030 | Batch 100/538 | Cost: 0.9371\n",
      "Epoch: 028/030 | Batch 120/538 | Cost: 1.0787\n",
      "Epoch: 028/030 | Batch 140/538 | Cost: 1.0542\n",
      "Epoch: 028/030 | Batch 160/538 | Cost: 0.9840\n",
      "Epoch: 028/030 | Batch 180/538 | Cost: 1.0318\n",
      "Epoch: 028/030 | Batch 200/538 | Cost: 1.0093\n",
      "Epoch: 028/030 | Batch 220/538 | Cost: 1.0424\n",
      "Epoch: 028/030 | Batch 240/538 | Cost: 1.0136\n",
      "Epoch: 028/030 | Batch 260/538 | Cost: 1.0332\n",
      "Epoch: 028/030 | Batch 280/538 | Cost: 1.0242\n",
      "Epoch: 028/030 | Batch 300/538 | Cost: 0.9527\n",
      "Epoch: 028/030 | Batch 320/538 | Cost: 1.0131\n",
      "Epoch: 028/030 | Batch 340/538 | Cost: 1.0367\n",
      "Epoch: 028/030 | Batch 360/538 | Cost: 1.0251\n",
      "Epoch: 028/030 | Batch 380/538 | Cost: 0.9915\n",
      "Epoch: 028/030 | Batch 400/538 | Cost: 1.0268\n",
      "Epoch: 028/030 | Batch 420/538 | Cost: 1.0311\n",
      "Epoch: 028/030 | Batch 440/538 | Cost: 1.0264\n",
      "Epoch: 028/030 | Batch 460/538 | Cost: 0.9708\n",
      "Epoch: 028/030 | Batch 480/538 | Cost: 1.0029\n",
      "Epoch: 028/030 | Batch 500/538 | Cost: 1.0028\n",
      "Epoch: 028/030 | Batch 520/538 | Cost: 1.0569\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.296\n",
      "  - uar:  36.828\n",
      "  - combined: 36.477\n",
      "Time elapsed: 24.91 min\n",
      "Epoch: 029/030 | Batch 000/538 | Cost: 0.9826\n",
      "Epoch: 029/030 | Batch 020/538 | Cost: 1.0009\n",
      "Epoch: 029/030 | Batch 040/538 | Cost: 1.0304\n",
      "Epoch: 029/030 | Batch 060/538 | Cost: 1.0036\n",
      "Epoch: 029/030 | Batch 080/538 | Cost: 1.0648\n",
      "Epoch: 029/030 | Batch 100/538 | Cost: 1.0074\n",
      "Epoch: 029/030 | Batch 120/538 | Cost: 0.9903\n",
      "Epoch: 029/030 | Batch 140/538 | Cost: 1.0495\n",
      "Epoch: 029/030 | Batch 160/538 | Cost: 1.0388\n",
      "Epoch: 029/030 | Batch 180/538 | Cost: 1.0415\n",
      "Epoch: 029/030 | Batch 200/538 | Cost: 1.0446\n",
      "Epoch: 029/030 | Batch 220/538 | Cost: 1.0657\n",
      "Epoch: 029/030 | Batch 240/538 | Cost: 1.0649\n",
      "Epoch: 029/030 | Batch 260/538 | Cost: 1.0032\n",
      "Epoch: 029/030 | Batch 280/538 | Cost: 0.9993\n",
      "Epoch: 029/030 | Batch 300/538 | Cost: 1.0146\n",
      "Epoch: 029/030 | Batch 320/538 | Cost: 1.0153\n",
      "Epoch: 029/030 | Batch 340/538 | Cost: 1.0424\n",
      "Epoch: 029/030 | Batch 360/538 | Cost: 1.0075\n",
      "Epoch: 029/030 | Batch 380/538 | Cost: 1.0384\n",
      "Epoch: 029/030 | Batch 400/538 | Cost: 0.9842\n",
      "Epoch: 029/030 | Batch 420/538 | Cost: 1.0417\n",
      "Epoch: 029/030 | Batch 440/538 | Cost: 1.0237\n",
      "Epoch: 029/030 | Batch 460/538 | Cost: 1.0167\n",
      "Epoch: 029/030 | Batch 480/538 | Cost: 0.9960\n",
      "Epoch: 029/030 | Batch 500/538 | Cost: 0.9801\n",
      "Epoch: 029/030 | Batch 520/538 | Cost: 1.0035\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.039\n",
      "  - uar:  36.635\n",
      "  - combined: 36.242\n",
      "Time elapsed: 25.80 min\n",
      "Epoch: 030/030 | Batch 000/538 | Cost: 1.0027\n",
      "Epoch: 030/030 | Batch 020/538 | Cost: 1.0249\n",
      "Epoch: 030/030 | Batch 040/538 | Cost: 0.9945\n",
      "Epoch: 030/030 | Batch 060/538 | Cost: 1.0042\n",
      "Epoch: 030/030 | Batch 080/538 | Cost: 1.0231\n",
      "Epoch: 030/030 | Batch 100/538 | Cost: 1.0084\n",
      "Epoch: 030/030 | Batch 120/538 | Cost: 1.0004\n",
      "Epoch: 030/030 | Batch 140/538 | Cost: 1.0010\n",
      "Epoch: 030/030 | Batch 160/538 | Cost: 1.0091\n",
      "Epoch: 030/030 | Batch 180/538 | Cost: 1.0089\n",
      "Epoch: 030/030 | Batch 200/538 | Cost: 1.0163\n",
      "Epoch: 030/030 | Batch 220/538 | Cost: 1.0612\n",
      "Epoch: 030/030 | Batch 240/538 | Cost: 0.9824\n",
      "Epoch: 030/030 | Batch 260/538 | Cost: 1.0027\n",
      "Epoch: 030/030 | Batch 280/538 | Cost: 1.0197\n",
      "Epoch: 030/030 | Batch 300/538 | Cost: 1.0295\n",
      "Epoch: 030/030 | Batch 320/538 | Cost: 1.0166\n",
      "Epoch: 030/030 | Batch 340/538 | Cost: 1.0304\n",
      "Epoch: 030/030 | Batch 360/538 | Cost: 0.9646\n",
      "Epoch: 030/030 | Batch 380/538 | Cost: 0.9898\n",
      "Epoch: 030/030 | Batch 400/538 | Cost: 1.0068\n",
      "Epoch: 030/030 | Batch 420/538 | Cost: 1.0457\n",
      "Epoch: 030/030 | Batch 440/538 | Cost: 0.9907\n",
      "Epoch: 030/030 | Batch 460/538 | Cost: 1.0230\n",
      "Epoch: 030/030 | Batch 480/538 | Cost: 1.0208\n",
      "Epoch: 030/030 | Batch 500/538 | Cost: 1.0322\n",
      "Epoch: 030/030 | Batch 520/538 | Cost: 1.0238\n",
      "Results in arousal:\n",
      "\n",
      "  - f1:  36.247\n",
      "  - uar:  36.782\n",
      "  - combined: 36.429\n",
      "Time elapsed: 26.69 min\n",
      "Total Training Time: 26.69 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "cost_list=[]\n",
    "schedule = [\n",
    "    (0, 10, 1e-4),\n",
    "    (10, 20, 1e-5),\n",
    "    (20, 30, 1e-6)]\n",
    "\n",
    "for start, end, lr in schedule:\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    for epoch in range(start, end):\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "            features = features.to(device).float()\n",
    "            targets = targets.to(device).long()\n",
    "            targets = targets.squeeze(1)\n",
    "\n",
    "            ### FORWARD AND BACK PROP\n",
    "            out = model(features)\n",
    "            \n",
    "            loss = criterion(out, targets)\n",
    "  \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            ### UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            #################################################\n",
    "            ### CODE ONLY FOR LOGGING BEYOND THIS POINT\n",
    "            ################################################\n",
    "            cost_list.append(loss.item())\n",
    "            if  not batch_idx % 20:\n",
    "                print (f'Epoch: {epoch+1:03d}/{end:03d} | '\n",
    "                       f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                       f' Cost: {loss:.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            pred=[]\n",
    "            targ=[]\n",
    "            for batch_idx, (features, targets) in enumerate(valid_loader):\n",
    "                features = features.to(device).float()\n",
    "                targets = targets.long()\n",
    "                out = model(features)\n",
    "                _, predicts = torch.max(out, 1)\n",
    "                predicts = predicts.cpu().detach().numpy()\n",
    "                targets = targets.cpu().detach().numpy()\n",
    "                pred.append(predicts)\n",
    "                targ.append(np.concatenate(targets))\n",
    "            targ = np.concatenate(targ)\n",
    "            pred = np.concatenate(pred)\n",
    "            eval_metric(pred, targ, 'arousal')\n",
    "        elapsed = (time.time() - start_time)/60\n",
    "        print(f'Time elapsed: {elapsed:.2f} min')\n",
    "\n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Total Training Time: {elapsed:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
